import asyncio
import aiohttp
import time
import re
import json
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, quote_plus
from app.config import settings

class OSINTCrawler:
    def __init__(self):
        self.session = None
        self.crawl_delay = settings.CRAWL_DELAY
        self.max_pages = settings.MAX_CRAWL_PAGES
        self.search_targets = []
        
        # í¬ë¡¤ë§ ì œì™¸ ì‚¬ì´íŠ¸ ëª©ë¡ (ë¡œê·¸ì¸/íšŒì›ê°€ì… í˜ì´ì§€ ë“±)
        self.excluded_paths = [
            '/login', '/signin', '/signup', '/register', '/join',
            '/auth', '/account', '/profile', '/settings',
            '/admin', '/moderator', '/dashboard'
        ]
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession(
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1',
                'Cache-Control': 'no-cache',
                'Pragma': 'no-cache',
            },
            timeout=aiohttp.ClientTimeout(total=15, connect=10)
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    def set_search_targets(self, email: Optional[str] = None, 
                          phone: Optional[str] = None, 
                          name: Optional[str] = None):
        """íƒìƒ‰ ëŒ€ìƒ ì„¤ì •"""
        self.search_targets = []
        
        if email:
            self.search_targets.append(('email', email))
        if phone:
            self.search_targets.append(('phone', phone))
        if name:
            self.search_targets.append(('name', name))
    
    def _should_skip_url(self, url: str) -> bool:
        """URLì´ í¬ë¡¤ë§ì—ì„œ ì œì™¸ë˜ì–´ì•¼ í•˜ëŠ”ì§€ í™•ì¸"""
        parsed = urlparse(url)
        
        # ì œì™¸ ê²½ë¡œ í™•ì¸
        for excluded_path in self.excluded_paths:
            if excluded_path in parsed.path.lower():
                return True
        
        # íŒŒì¼ í™•ì¥ì í™•ì¸ (ì´ë¯¸ì§€, PDF ë“± ì œì™¸)
        excluded_extensions = ['.jpg', '.jpeg', '.png', '.gif', '.pdf', '.doc', '.docx', '.xls', '.xlsx']
        for ext in excluded_extensions:
            if parsed.path.lower().endswith(ext):
                return True
        
        return False
    
    async def search_google_dorks(self) -> List[Dict]:
        """Google Dork ê²€ìƒ‰"""
        results = []
        
        for target_type, target_value in self.search_targets:
            dorks = self._generate_google_dorks(target_type, target_value)
            
            for dork in dorks:
                try:
                    search_url = f"https://www.google.com/search?q={quote_plus(dork)}"
                    async with self.session.get(search_url) as response:
                        if response.status == 200:
                            content = await response.text()
                            found_patterns = self._search_patterns_in_text(content)
                            
                            for pattern_type, value, context in found_patterns:
                                results.append({
                                    'source_url': search_url,
                                    'pattern_type': pattern_type,
                                    'value': value,
                                    'context': context,
                                    'timestamp': time.time(),
                                    'search_method': 'google_dork'
                                })
                    
                    await asyncio.sleep(self.crawl_delay)
                    
                except Exception as e:
                    print(f"Google Dork ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        
        return results
    
    def _generate_google_dorks(self, target_type: str, target_value: str) -> List[str]:
        """Google Dork ì¿¼ë¦¬ ìƒì„±"""
        dorks = []
        
        if target_type == 'email':
            dorks.extend([
                f'"{target_value}"',
                f'"{target_value}" filetype:txt',
                f'"{target_value}" site:pastebin.com',
                f'"{target_value}" site:github.com',
                f'"{target_value}" intext:"password"',
                f'"{target_value}" intext:"leak"',
            ])
        elif target_type == 'phone':
            dorks.extend([
                f'"{target_value}"',
                f'"{target_value}" filetype:txt',
                f'"{target_value}" site:pastebin.com',
                f'"{target_value}" intext:"ê°œì¸ì •ë³´"',
            ])
        elif target_type == 'name':
            dorks.extend([
                f'"{target_value}"',
                f'"{target_value}" filetype:txt',
                f'"{target_value}" intext:"ê°œì¸ì •ë³´"',
                f'"{target_value}" intext:"ìœ ì¶œ"',
            ])
        
        return dorks
    
    async def crawl_forum_sites(self) -> List[Dict]:
        """í¬ëŸ¼ ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
        forum_sites = [
            'https://www.reddit.com',
            'https://stackoverflow.com',
            'https://github.com',
            'https://paste.ee',
            'https://www.clien.net',
            'https://www.dcinside.com',
            'https://www.inven.co.kr',
            'https://www.ruliweb.com',
            'https://www.ppomppu.co.kr',
            'https://www.fmkorea.com',
            'https://www.82cook.com'
        ]
        
        results = []
        
        for site in forum_sites:
            try:
                print(f"ğŸ“ í¬ëŸ¼ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì¤‘: {site}")
                site_results = await self._crawl_site(site)
                results.extend(site_results)
                print(f"âœ… {site} í¬ë¡¤ë§ ì™„ë£Œ: {len(site_results)}ê°œ ê²°ê³¼")
                await asyncio.sleep(self.crawl_delay)
            except Exception as e:
                print(f"âŒ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨ {site}: {e}")
                continue
        
        return results

    async def crawl_data_leak_sites(self) -> List[Dict]:
        """ë°ì´í„° ìœ ì¶œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
        leak_sites = [
            'https://haveibeenpwned.com',
            'https://breachdirectory.pw',
            'https://leakcheck.io',
            'https://dehashed.com',
            'https://intelx.io',
            'https://snusbase.com',
            'https://leak-lookup.com',
            'https://weleakinfo.com',
            'https://leakcheck.net',
            'https://leakpeek.com'
        ]
        
        results = []
        
        for site in leak_sites:
            try:
                print(f"ğŸ” ë°ì´í„° ìœ ì¶œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì¤‘: {site}")
                site_results = await self._crawl_site(site)
                results.extend(site_results)
                print(f"âœ… {site} í¬ë¡¤ë§ ì™„ë£Œ: {len(site_results)}ê°œ ê²°ê³¼")
                await asyncio.sleep(self.crawl_delay)
            except Exception as e:
                print(f"âŒ ë°ì´í„° ìœ ì¶œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨ {site}: {e}")
                continue
        
        return results

    async def crawl_paste_sites(self) -> List[Dict]:
        """í˜ì´ìŠ¤íŠ¸ ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
        paste_sites = [
            'https://pastebin.com',
            'https://paste.ee',
            'https://rentry.co',
            'https://paste.rs',
            'https://paste.gg',
            'https://paste.fo',
            'https://paste.ubuntu.com',
            'https://paste.debian.net',
            'https://paste.kde.org',
            'https://paste.opensuse.org'
        ]
        
        results = []
        
        for site in paste_sites:
            try:
                print(f"ğŸ“‹ í˜ì´ìŠ¤íŠ¸ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì¤‘: {site}")
                site_results = await self._crawl_site(site)
                results.extend(site_results)
                print(f"âœ… {site} í¬ë¡¤ë§ ì™„ë£Œ: {len(site_results)}ê°œ ê²°ê³¼")
                await asyncio.sleep(self.crawl_delay)
            except Exception as e:
                print(f"âŒ í˜ì´ìŠ¤íŠ¸ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨ {site}: {e}")
                continue
        
        return results
    
    async def crawl_social_media(self) -> List[Dict]:
        """ì†Œì…œ ë¯¸ë””ì–´ í¬ë¡¤ë§"""
        social_sites = [
            'https://twitter.com',
            'https://www.facebook.com',
            'https://www.instagram.com',
            'https://www.linkedin.com'
        ]
        
        results = []
        
        for site in social_sites:
            try:
                print(f"ğŸ“± ì†Œì…œ ë¯¸ë””ì–´ í¬ë¡¤ë§ ì¤‘: {site}")
                site_results = await self._crawl_site(site)
                results.extend(site_results)
                print(f"âœ… {site} í¬ë¡¤ë§ ì™„ë£Œ: {len(site_results)}ê°œ ê²°ê³¼")
                await asyncio.sleep(self.crawl_delay)
            except Exception as e:
                print(f"âŒ ì†Œì…œ ë¯¸ë””ì–´ í¬ë¡¤ë§ ì‹¤íŒ¨ {site}: {e}")
                continue
        
        return results
    
    async def crawl_dark_web_sources(self) -> List[Dict]:
        """ë‹¤í¬ì›¹ ì†ŒìŠ¤ í¬ë¡¤ë§ (ì‹œë®¬ë ˆì´ì…˜)"""
        # ì‹¤ì œ ë‹¤í¬ì›¹ ì ‘ê·¼ì€ ë³„ë„ API í•„ìš”
        dark_web_sites = [
            'http://example.onion',  # ì‹œë®¬ë ˆì´ì…˜ìš©
            'http://test.onion'      # ì‹œë®¬ë ˆì´ì…˜ìš©
        ]
        
        results = []
        
        for site in dark_web_sites:
            try:
                # ì‹¤ì œë¡œëŠ” TOR ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•œ ì ‘ê·¼ í•„ìš”
                site_results = await self._crawl_site(site)
                results.extend(site_results)
                await asyncio.sleep(self.crawl_delay)
            except Exception as e:
                print(f"ë‹¤í¬ì›¹ í¬ë¡¤ë§ ì‹¤íŒ¨ {site}: {e}")
        
        return results
    
    async def _crawl_site(self, base_url: str) -> List[Dict]:
        """íŠ¹ì • ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
        results = []
        
        try:
            # ë©”ì¸ í˜ì´ì§€ í¬ë¡¤ë§
            async with self.session.get(base_url) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # HTMLì´ ì•„ë‹Œ ê²½ìš° ìŠ¤í‚µ
                    if not content.strip().startswith('<'):
                        return results
                    
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # í…ìŠ¤íŠ¸ì—ì„œ ê°œì¸ì •ë³´ íŒ¨í„´ ê²€ìƒ‰
                    found_patterns = self._search_patterns_in_text(content)
                    
                    for pattern_type, value, context in found_patterns:
                        results.append({
                            'source_url': base_url,
                            'pattern_type': pattern_type,
                            'value': value,
                            'context': context,
                            'timestamp': time.time(),
                            'search_method': 'direct_crawl'
                        })
                    
                    # ë§í¬ ì¶”ì¶œ ë° ì¶”ê°€ í˜ì´ì§€ í¬ë¡¤ë§
                    links = self._extract_links(soup, base_url)
                    
                    # ì œì™¸í•  ë§í¬ í•„í„°ë§
                    filtered_links = [link for link in links if not self._should_skip_url(link)]
                    
                    for link in filtered_links[:self.max_pages]:
                        try:
                            link_results = await self._crawl_page(link)
                            results.extend(link_results)
                            await asyncio.sleep(self.crawl_delay)
                        except Exception as e:
                            print(f"ë§í¬ í¬ë¡¤ë§ ì‹¤íŒ¨ {link}: {e}")
                            continue
                            
        except asyncio.TimeoutError:
            print(f"ì‚¬ì´íŠ¸ í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ: {base_url}")
        except aiohttp.ClientError as e:
            print(f"ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì—°ê²° ì˜¤ë¥˜ {base_url}: {e}")
        except Exception as e:
            print(f"ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì˜¤ë¥˜ {base_url}: {e}")
        
        return results
    
    async def _crawl_page(self, url: str) -> List[Dict]:
        """ë‹¨ì¼ í˜ì´ì§€ í¬ë¡¤ë§"""
        results = []
        
        # URL ì œì™¸ í™•ì¸
        if self._should_skip_url(url):
            return results
        
        try:
            async with self.session.get(url, timeout=10) as response:
                if response.status == 200:
                    content = await response.text()
                    
                    # HTMLì´ ì•„ë‹Œ ê²½ìš° ìŠ¤í‚µ
                    if not content.strip().startswith('<'):
                        return results
                    
                    found_patterns = self._search_patterns_in_text(content)
                    
                    for pattern_type, value, context in found_patterns:
                        results.append({
                            'source_url': url,
                            'pattern_type': pattern_type,
                            'value': value,
                            'context': context,
                            'timestamp': time.time(),
                            'search_method': 'page_crawl'
                        })
                        
        except asyncio.TimeoutError:
            print(f"í˜ì´ì§€ í¬ë¡¤ë§ íƒ€ì„ì•„ì›ƒ: {url}")
        except aiohttp.ClientError as e:
            print(f"í˜ì´ì§€ í¬ë¡¤ë§ ì—°ê²° ì˜¤ë¥˜ {url}: {e}")
        except Exception as e:
            print(f"í˜ì´ì§€ í¬ë¡¤ë§ ì˜¤ë¥˜ {url}: {e}")
        
        return results
    
    def _search_patterns_in_text(self, text: str) -> List[tuple]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê°œì¸ì •ë³´ íŒ¨í„´ ê²€ìƒ‰"""
        found_patterns = []
        
        for target_type, target_value in self.search_targets:
            if target_type == 'email':
                # ì´ë©”ì¼ íŒ¨í„´ ê²€ìƒ‰
                email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
                emails = re.findall(email_pattern, text)
                
                for email in emails:
                    if email.lower() == target_value.lower():
                        context = self._extract_context(text, email, 100)
                        found_patterns.append(('email', email, context))
            
            elif target_type == 'phone':
                # ì „í™”ë²ˆí˜¸ íŒ¨í„´ ê²€ìƒ‰ (í•œêµ­ ì „í™”ë²ˆí˜¸ í¬í•¨)
                phone_patterns = [
                    r'\b\d{3}-\d{3,4}-\d{4}\b',  # 010-1234-5678
                    r'\b\d{10,11}\b',  # 01012345678
                    r'\b\+82-?\d{1,2}-?\d{3,4}-?\d{4}\b',  # +82-10-1234-5678
                    r'\b01[016789]-\d{3,4}-\d{4}\b',  # í•œêµ­ íœ´ëŒ€í°
                    r'\b0[2-9]{1,2}-\d{3,4}-\d{4}\b',  # í•œêµ­ ì¼ë°˜ì „í™”
                ]
                
                for pattern in phone_patterns:
                    phones = re.findall(pattern, text)
                    for phone in phones:
                        normalized_phone = re.sub(r'[^\d]', '', phone)
                        normalized_target = re.sub(r'[^\d]', '', target_value)
                        
                        if normalized_phone == normalized_target:
                            context = self._extract_context(text, phone, 100)
                            found_patterns.append(('phone', phone, context))
            
            elif target_type == 'name':
                # ì´ë¦„ íŒ¨í„´ ê²€ìƒ‰ (í•œêµ­ ì´ë¦„ í¬í•¨)
                name_patterns = [
                    rf'\b{re.escape(target_value)}\b',  # ì •í™•í•œ ë§¤ì¹­
                    rf'\b{re.escape(target_value)}[ì”¨ë‹˜]\b',  # í•œêµ­ í˜¸ì¹­
                    rf'\b{re.escape(target_value)}[ê°€-í£]*\b',  # í•œêµ­ ì´ë¦„ í™•ì¥
                ]
                
                for pattern in name_patterns:
                    names = re.findall(pattern, text, re.IGNORECASE)
                    for name in names:
                        context = self._extract_context(text, name, 100)
                        found_patterns.append(('name', name, context))
        
        return found_patterns
    
    def _extract_context(self, text: str, found_value: str, context_length: int) -> str:
        """ë°œê²¬ëœ ê°’ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        try:
            index = text.find(found_value)
            if index != -1:
                start = max(0, index - context_length)
                end = min(len(text), index + len(found_value) + context_length)
                context = text[start:end].replace('\n', ' ').strip()
                return context
        except:
            pass
        return found_value
    
    def _extract_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """í˜ì´ì§€ì—ì„œ ë§í¬ ì¶”ì¶œ"""
        links = []
        
        for link in soup.find_all('a', href=True):
            href = link['href']
            full_url = urljoin(base_url, href)
            
            # ê°™ì€ ë„ë©”ì¸ì˜ ë§í¬ë§Œ ìˆ˜ì§‘
            if urlparse(full_url).netloc == urlparse(base_url).netloc:
                links.append(full_url)
        
        return links[:self.max_pages]
    
    async def crawl_all_sources(self) -> List[Dict]:
        """ëª¨ë“  ì†ŒìŠ¤ì—ì„œ í¬ë¡¤ë§ ìˆ˜í–‰"""
        results = []
        
        if not self.search_targets:
            print("âš ï¸ íƒìƒ‰ ëŒ€ìƒì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return results
        
        print(f"ğŸ” íƒìƒ‰ ëŒ€ìƒ: {[f'{t[0]}:{t[1]}' for t in self.search_targets]}")
        
        # Google Dork ê²€ìƒ‰
        print("ğŸ” Google Dork ê²€ìƒ‰ ì¤‘...")
        google_results = await self.search_google_dorks()
        results.extend(google_results)
        print(f"âœ… Google Dork ê²€ìƒ‰ ì™„ë£Œ: {len(google_results)}ê°œ ê²°ê³¼")
        
        # ë°ì´í„° ìœ ì¶œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        print("ğŸ” ë°ì´í„° ìœ ì¶œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì¤‘...")
        leak_results = await self.crawl_data_leak_sites()
        results.extend(leak_results)
        print(f"âœ… ë°ì´í„° ìœ ì¶œ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: {len(leak_results)}ê°œ ê²°ê³¼")
        
        # í˜ì´ìŠ¤íŠ¸ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
        print("ğŸ“‹ í˜ì´ìŠ¤íŠ¸ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì¤‘...")
        paste_results = await self.crawl_paste_sites()
        results.extend(paste_results)
        print(f"âœ… í˜ì´ìŠ¤íŠ¸ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: {len(paste_results)}ê°œ ê²°ê³¼")
        
        # í¬ëŸ¼ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
        print("ğŸ“ í¬ëŸ¼ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì¤‘...")
        forum_results = await self.crawl_forum_sites()
        results.extend(forum_results)
        print(f"âœ… í¬ëŸ¼ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: {len(forum_results)}ê°œ ê²°ê³¼")
        
        # ì†Œì…œ ë¯¸ë””ì–´ í¬ë¡¤ë§
        print("ğŸ“± ì†Œì…œ ë¯¸ë””ì–´ í¬ë¡¤ë§ ì¤‘...")
        social_results = await self.crawl_social_media()
        results.extend(social_results)
        print(f"âœ… ì†Œì…œ ë¯¸ë””ì–´ í¬ë¡¤ë§ ì™„ë£Œ: {len(social_results)}ê°œ ê²°ê³¼")
        
        # ë¸”ë¡œê·¸ ì‚¬ì´íŠ¸ í¬ë¡¤ë§
        print("ğŸ“– ë¸”ë¡œê·¸ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì¤‘...")
        blog_results = await self.crawl_blog_sites()
        results.extend(blog_results)
        print(f"âœ… ë¸”ë¡œê·¸ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì™„ë£Œ: {len(blog_results)}ê°œ ê²°ê³¼")
        
        print(f"ğŸ¯ ì´ í¬ë¡¤ë§ ê²°ê³¼: {len(results)}ê°œ")
        return results
    
    async def crawl_blog_sites(self) -> List[Dict]:
        """ë¸”ë¡œê·¸/ë‰´ìŠ¤ ì‚¬ì´íŠ¸ í¬ë¡¤ë§"""
        blog_sites = [
            'https://medium.com',
            'https://dev.to',
            'https://hashnode.dev',
            'https://velog.io',
            'https://tistory.com',
            'https://blog.naver.com',
            'https://brunch.co.kr'
        ]
        
        results = []
        
        for site in blog_sites:
            try:
                site_results = await self._crawl_site(site)
                results.extend(site_results)
                await asyncio.sleep(self.crawl_delay)
            except Exception as e:
                print(f"ë¸”ë¡œê·¸ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹¤íŒ¨ {site}: {e}")
        
        return results 